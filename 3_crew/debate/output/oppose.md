Oppose the motion. Strict, one-size-fits-all statutes would freeze a technology whose greatest dangers are still speculative and whose greatest benefits—life-saving medical triage, real-time translation for crisis responders, personalized education for every child—are already materializing. LLMs evolve weekly; rigid compliance calendars written today would outlaw tomorrow’s safer architectures. Meanwhile, the feared harms—deep-fake fraud, biased medical advice, automated malware—are already covered by existing law: fraud, malpractice, and computer-misuse statutes apply regardless of whether an LLM, Photoshop, or a human hand committed the act. Layering new “LLM-specific” criminal liability on top creates a moving-target compliance nightmare that only the largest incumbents can afford, entrenching their dominance and suffocating the open-source community that currently provides the most transparent, auditable models. Contrast this with the flexible co-regulation that governs aviation and pharmaceuticals: government sets outcomes (safety, efficacy), private labs continuously prove compliance, and standards evolve with the science. We should demand that LLM deployers carry insurance proportional to model capability, publish standardized risk assessments, and open their weights to licensed auditors—enforceable through civil penalties, not blanket criminal bans. This approach keeps the liability where it belongs: on the human actor who chooses to misuse the tool, not on the mathematics of language itself.